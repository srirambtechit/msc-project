Dataset preprocessing:
   As our problem is revolved around object detection and usage of Detectron2 framework, leave us with the option of COCO dataset format. Metadata collected for images is written in COCO format in the _annotation.coco.json.
   
   Though we have collected approx 450 images, that is not good enough to cover all scenarios. The reason because sometimes the elements that the model try to identify the pattern on training /*may not be having a good lighting condition, may have a cut, fliped videos*/ may have cut the important elements of the frame to identify umpire from video for instance umpire's head, hand, leg or body due to an interest of tracking ball, batsman, bowler by the camera. So, we decided to incorportate Data Agumentation technique to generate more variants of data just for trainig dataset. 70% of our data used for training from the existing 450 images using the following techniques. So overall, approx more than 1000 images were generated after agumentation.
   
   Flip: Trick of changing the image position either horizontal or vertical. Horizontal flipping is enough as the vertical flipping will not provide much improvment. Mostly the camera angle would be in horizontal and generated videos will be on the same way. Hence vertical flipping ruled out

   Crop: It adds variability to positioning and size to help our model to be more resilient to subject translations and camera position. We've adjusted the range between 0% minimum zoom level, 32% maximum zoom

   Blur: Adding random Gaussian blur to help our model be more resilient to camera focus and adjusted upto 2.5 pixel.

   Grayscale: Probabilistically apply grayscale to a subset of training set. Here, 12% of outputted images to Grayscale.

   Auto-orient: Discarding EXIF rotations and standardize pixel ordering is paramount important for any computer vision problem.

   Resize: Downsizing images to 416 * 146 for smaller files and faster training

   Auto-adjust-contrast: Boosting contrasts based on the images's histogram to improve normalization and line detection in varying lighting conditions. Adaptive equalization is used on this preprocessing task.

   Umpire Pose Model - 442 images 1062 after augmentation
   Umpire Model      - 452 images 1086 after augmentation

   Data splitted as 70%, 20%, 10% for training, validation and test respecetively for both models.

Implementation:
  We have selected Faster R-CNN model for our case study. The solution to the problem is approached in 3 phased manner. Firstly, detection of umpire vs non-umpire, using the model selected earlier to train the dataset and lets call it as "Umpire Classifier" (UC)model. Secondly, detection of different pose or signs umpire shows during the match (SIX, NO-BALL, OUT, WIDE and NO-ACTION), and we  named as "Umpire Pose Classifier" (UPC) model, again using the same network but trained separately.

  Training a neural network is time consuming process, and requires a huge amount of compute (GPU and Memeory) in order perform the training process again. Having said that we've used transfered learning method to save the weights of the models (UCM, UPCM) and tuned parameters it got trained on to a model-final.pth format that detectron2 framework can understand and load later on the timeline. 

  Finally, Python based main module program is called for integrating both UC and UPC by loading the models from the persistent storage to GPU for the prediction in order to find umpire and umpire signs on the video data.
  
  There are two Faster R-CNN models were trained separately.
  1. Umpire vs Nonumpire detection classifier (UC)
  2. Umpire pose/signs classifier (UPC)

Clip Filtration and Highlights generation:
  Detectron2 returns the prediction result as Instance object which consist of number of instances matched, instance boundary(xmin, ymin, xmax, ymax points), pred_classes (labels like umpire, non-umpire, six, out, etc.) and score for the prediction. Highlights generation achieved by extracting predictions of UC model by identifying umpire on the scene of the video frame and discarding other frames. Eventually, the selected frame (by UC model) is fed into UPC model to detect poses/signals other "NO-ACTION" to decide that particular frame needed for our hightlights.

6.Evaluation Metric:
  Evaulation of the models were done separately as the model trained separately. Below metrics would help us understand the models accuracy. Object detection work needs to find average precision and average recall to get a better insight of how the model performed.
  1) Umpire Classifier Model
     1) Dataset used to train/valid/test this model as tabulated below
     Total images 1086
     Train    Valid   Test
     951      90      45

     2) Distribution of instances among all three categories
     Phase      Umpire Non-Umpire Total
     Training   513    438        951
     Validation 49     41         90
     Testing    18     17         45

     3) Evaluation result for bounded box in each iteration as follows
     Iterations  AP     AP50   AP75   APs  APm  APl
     1000th      90.695 93.387 93.387 NA   NA   90.716
     1500th      90.351 96.448 96.448 NA   NA   96.351

  2) Umpire Pose Classifier Model:
     1) Dataset used to train/valid/test this model as tabulated below
     Total images 1063
     Train    Valid   Test
     930      88      45

     2) Distribution of instances among all three categories
     Phase      No-Action  Out  Six  No-Ball  Wide  Total
     Training   147        213  198  201      177   936
     Validation 16         17   17   12       26    88
     Testing    14         6    8    11       6     45

     3) Evaluation result for bounded box in each iteration as follows
     Iterations  AP     AP50   AP75   APs  APm  APl
     1000th      72.789 89.103 86.934 NA   NA   72.794
     1500th      86.961 93.412 90.969 NA   NA   86.971
  
Results & Analysis:
Based on the evaulation and models performance, we got pretty good accuracy on both the models. Finally result of the combined models was convincing us in generated video.
1) Umpire Classifier Model
Final result after evaulation of umpire classifier model
     Category    AP
     non-umpire  96.118
     umpire      90.822

2) Umpire Pose Classifier Model:
Final result after evaulation of umpire pose classifier model
     Category    AP
     no-action   79.736
     out         89.663
     six         92.700
     no-ball     65.416
     wide        98.317

Our model takes approx 3.4hrs to generate the highlights on Tesla T40 GPU (colab used here) for input video of 4 minutes, 1280 × 720 resolution, 30 fps and mp4 format. Model produced video length was 53 seconds (depends on the no. of events and detection of umpire signs).

Future Work:
  In this research work, we have identified the key elements like umpire and their pose in video frame and extracted those frames alone based on the prediction done by our model to generate highlights. As part of future work, there are other signs of umpire like Bye, Leg Bye, One Short, Dead Ball, Cancel Call, New Ball, Penalty Runs would be included in the training process with new set of annotated data to get more interesting elements of the match. Since this topic is on highlights generation, we can also leverage this work as an object detection task to include bat, ball, stump, batsman, bowler and player positions to detect and classify the video frame - a catch, run out, stumped, bowled or hit wicket for the batsman being dismissed; These are few of interesting elements of the game to build as a full-blown solution.

